{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgiYK3wfQXjr"
      },
      "source": [
        "Welcome to your assignment about concepts covered in Chapter 7 of *Essential Math for Data Science* by Thomas Nield. You will be exploring neural networks in this assignment.\n",
        "\n",
        "Please read each question carefully and provide detailed explanations for your answers, including any relevant calculations or work. You are also required to provide Python solutions for the technical problems in each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NPMNlgAQUoG"
      },
      "source": [
        "# Problem 1 Forward Propagation\n",
        "\n",
        "Implement the forward propagation algorithm for a simple neural network with one hidden layer. The neural network has the following specifications:\n",
        "\n",
        "1. Input layer with 3 features\n",
        "\n",
        "2. Hidden layer with 4 units, using the ReLU activation function\n",
        "\n",
        "3. Output layer with 2 units, using the softmax activation function\n",
        "\n",
        "Write a Python function forward_propagation that takes an input array X of shape (m, 3), where m is the number of examples, and returns the output predictions of shape (m, 2). Assume the weights and biases of the neural network are pre-defined.\n",
        "\n",
        "\n",
        "Notes (more detailed explanation of the question):\n",
        "\n",
        " The forward propagation algorithm for a simple neural network with one hidden layer involves passing the input data through the network to make predictions. This neural network has three layers: an input layer with 3 features, a hidden layer with 4 units (neurons), and an output layer with 2 units. Each unit in the hidden layer uses the ReLU activation function, while each unit in the output layer uses the softmax activation function.\n",
        "\n",
        "The purpose of the forward propagation algorithm is to take the input data and compute the predicted output of the neural network. The input data comes in the form of an array called X, where each row represents an example, and there are three columns representing the three features. The goal is to calculate the predictions for each example and return the results in an array of shape (m, 2), where m is the number of examples, and 2 represents the two units in the output layer (the classes for the binary classification problem).\n",
        "\n",
        "To implement this algorithm in Python, you can create a function called forward_propagation. This function takes the input array X as its input, and you should assume that the weights and biases of the neural network have been pre-defined. These weights and biases determine how the input data will be transformed as it passes through the neural network to produce the predictions.\n",
        "\n",
        "The steps involved in forward propagation are as follows:\n",
        "\n",
        "Take the input X and compute the values of the hidden layer units using the ReLU activation function.\n",
        "Use the weights and biases of the connections between the input layer and the hidden layer to calculate the values of the hidden units.\n",
        "Once the values of the hidden units are calculated, apply the softmax activation function to calculate the predictions of the output layer.\n",
        "Return the predictions for each example in the form of an array of shape (m, 2).\n",
        "After implementing the forward_propagation function, you can use it to make predictions on new data using the pre-defined weights and biases of the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcOJK71EQi_m"
      },
      "source": [
        "Step 1. Import numpy as np. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UsCO0G4cO7Un"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJL-8IhuO8jY"
      },
      "source": [
        "Step 2. Define the weights and biases in your forward propagation. We have predefined weights and biases below. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VAELOsNXPGIz"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X):\n",
        "    # Define the weights and biases\n",
        "    W1 = np.array([[0.1, 0.2, 0.3, 0.4],\n",
        "                   [0.5, 0.6, 0.7, 0.8],\n",
        "                   [0.9, 1.0, 1.1, 1.2]])\n",
        "\n",
        "    b1 = np.array([0.1, 0.2, 0.3, 0.4])\n",
        "\n",
        "    W2 = np.array([[0.5, 0.6],\n",
        "                   [0.7, 0.8],\n",
        "                   [0.9, 1.0],\n",
        "                   [1.1, 1.2]])\n",
        "\n",
        "    b2 = np.array([0.5, 0.6])\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = np.maximum(0, Z1)  # ReLU activation\n",
        "\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    exp_scores = np.exp(Z2)\n",
        "    A2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # Softmax activation\n",
        "\n",
        "    return A2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp_4ja5_SG09"
      },
      "source": [
        "Step 3. Enter different numbers (ranging from 1-10) in the arrays (3 each) below to see what the prediction will be. Feel free to experiment by repeating an array to see what happens with the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qFlxCDvFO8wO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00458669 0.99541331]\n",
            " [0.0021078  0.9978922 ]]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[1, 5, 9],\n",
        "              [2, 6, 10]])\n",
        "predictions1 = forward_propagation(X)\n",
        "print(predictions1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ohv3_Tu9W9EG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00398987 0.99601013]\n",
            " [0.04926601 0.95073399]]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[3, 8, 7],\n",
        "              [4, 1, 5]])\n",
        "predictions2 = forward_propagation(X)\n",
        "print(predictions2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TUQVY1cTXKIS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.00172638 0.99827362]\n",
            " [0.01536771 0.98463229]]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[4, 6, 10],\n",
        "              [2, 8, 4]])\n",
        "predictions3 = forward_propagation(X)\n",
        "print(predictions3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWSyysYyQVPd"
      },
      "source": [
        "# Problem 2\n",
        "Task: Use a neural network in Python to solve a classification problem using the given dataset. Follow the example in this link: https://www.analyticsvidhya.com/blog/2021/10/implementing-artificial-neural-networkclassification-in-python-from-scratch/\n",
        "A pdf is available under Supportive Learning Materials in Week 8.\n",
        "\n",
        "Dataset Description:\n",
        "We have a dataset from the finance domain with 100,000 records and 14 dimensions. The dimensions include RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, and Exited. The goal is to create an artificial neural network that predicts whether a customer will exit the bank or not based on the given independent variables.\n",
        "\n",
        "We have provided the Churn_Modelling.csv file, you need to upload it to your Google drive so you are able to access it for this problem. The file was downloaded from:\n",
        "https://www.kaggle.com/datasets/aakash50897/churn-modellingcsv?resource=download\n",
        "\n",
        "Follow the steps provided, making changes to the code when requested. Then change the outer layer activation to compare the accuracies of these three outer layer activation: sigmoid, softmax and exponential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO_6S485MSN6"
      },
      "source": [
        "Step 1: Import the necessary libraries: numpy, pandas, and tensorflow. Numpy library for numerical operations in Python, which provides support for handling arrays and matrices efficiently. Pandas: A library for data manipulation and analysis, allowing easy handling of structured data. TensorFlow: An open-source machine learning library developed by Google, used for building and training neural networks. *Nothing to change in the code below.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tFbexmE0MSYN"
      },
      "outputs": [],
      "source": [
        "#Importing necessary Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_nLBRwbMShv"
      },
      "source": [
        "Step 2: Load the dataset using the read_csv() method from pandas. In this step, we read the dataset from a CSV file into a pandas DataFrame using the read_csv() method. The dataset is usually organized in a tabular format with rows representing examples (data points) and columns representing features. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HyKQZmNMSpf",
        "outputId": "120cbb05-af2a-4a00-bc28-d6b8fca132a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age   \n",
            "0          1    15634602  Hargrave          619    France  Female   42  \\\n",
            "1          2    15647311      Hill          608     Spain  Female   41   \n",
            "2          3    15619304      Onio          502    France  Female   42   \n",
            "3          4    15701354      Boni          699    France  Female   39   \n",
            "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
            "\n",
            "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember   \n",
            "0       2       0.00              1          1               1  \\\n",
            "1       1   83807.86              1          0               1   \n",
            "2       8  159660.80              3          1               0   \n",
            "3       1       0.00              2          0               0   \n",
            "4       2  125510.82              1          1               1   \n",
            "\n",
            "   EstimatedSalary  Exited  \n",
            "0        101348.88       1  \n",
            "1        112542.58       0  \n",
            "2        113931.57       1  \n",
            "3         93826.63       0  \n",
            "4         79084.10       0  \n"
          ]
        }
      ],
      "source": [
        "#this is to set the file path for Python to access the dataset under Files in your Google Drive\n",
        "# You need to upload the Churn_Modelling.csv into your \"My Drive\", on the upper left corner of your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#Loading Dataset\n",
        "file_path = '/content/drive/MyDrive/Churn_Modelling.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "# Displaying the loaded data\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRiAgqDcMSzN"
      },
      "source": [
        "Step 3: Generate the matrix of features (X) by excluding the first three columns and the last column from the dataset. The feature matrix X is created by selecting the relevant columns from the dataset, excluding the first three columns (which might contain some identifier or irrelevant information) and the last column (which contains the dependent variable, the one we want to predict).\n",
        "*We are using \"iloc\" below from pandas data frame to allow us to fetch the desired columns. If you look at the data, you will see that the first three columns are RowNumber, CustomerID, and Surname. None of these columns are necessary in the analysis so we are ignoring them and starting with \"Credit Score\". We also don't want to include our target variable (last column) in the matrix, so we use -1 to exclude it*. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fn68lMxHMS8k"
      },
      "outputs": [],
      "source": [
        "#Generating Matrix of Features (X)\n",
        "X = data.iloc[:, 3:-1].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osu7VMUdMTHz"
      },
      "source": [
        "Step 4: Generate the dependent variable vector (Y) by selecting the last column from the dataset. In this step, we create the dependent variable vector Y by selecting the last column from the dataset. This column represents the target variable or the labels we want to predict. *We are using iloc to pick the last column \"exited\" as our target variable, this represented by -1*. *Nothing to change in the code below.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4AlWq8mWMTQj"
      },
      "outputs": [],
      "source": [
        "#Generating Dependent Variable Vector(Y)\n",
        "Y = data.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jqzlbKhMTbJ"
      },
      "source": [
        "Step 5: Encode the categorical variable \"Gender\" using label encoding. Label encoding is used to convert categorical data (like \"Gender\") into numerical format. It assigns a unique integer to each category in the \"Gender\" column, making it easier for the neural network to process. Neural networks perform best when their inputs are digits rather than strings. So we encode it to make sure the model can use it. We have two genders, so we use 2. If we had more than 2, we would add more numbers to match the number of categories.*Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9sj3SbSaMTjm"
      },
      "outputs": [],
      "source": [
        "#Encoding Categorical Variable Gender\n",
        "LE1 = LabelEncoder()\n",
        "X[:, 2] = np.array(LE1.fit_transform(X[:, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRwn6k55MTuy"
      },
      "source": [
        "Step 6: Encode the categorical variable \"Geography\" using one-hot encoding. One-hot encoding is another technique for handling categorical data. It creates binary columns for each category in the \"Geography\" column, representing the presence or absence of each category. Spain will be encoded as 001, France will be 010, and Germany is 100. This helps to prevent the neural network from thinking that higher numbers are more important. We don't want to introduce bias accidentially... *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-GvTUdTNMT3L"
      },
      "outputs": [],
      "source": [
        "#Encoding Categorical Variable Country\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder=\"passthrough\")\n",
        "X = np.array(ct.fit_transform(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZFetbLgMUCD"
      },
      "source": [
        "Step 7: Split the dataset into training and testing datasets using an 80:20 ratio. To evaluate the performance of the neural network, we need to split the dataset into training and testing datasets. The training dataset is used to train the model, while the testing dataset is used to evaluate its performance on unseen data. *Set the test size in the parameters below where you see a question mark(?).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0NbfdnVBMULk"
      },
      "outputs": [],
      "source": [
        "#Splitting Dataset into Training and Testing Dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ7BMTJuMUV4"
      },
      "source": [
        "Step 8: Perform feature scaling on the training and testing datasets using standardization. Feature scaling is a process of normalizing the features to ensure they all have a similar scale. Standardization is a common method that scales the features to have a mean of 0 and a standard deviation of 1. It helps the neural network to converge faster during training. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "u3uhdzwlMUfo"
      },
      "outputs": [],
      "source": [
        "#Performing Feature Scaling\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s87MnypMUr5"
      },
      "source": [
        "Step 9: Initialize the artificial neural network using the Sequential class from the Keras library. In this step, we create an instance of the Sequential class from the Keras library. The Sequential class allows us to build a neural network by stacking layers on top of each other in a linear fashion. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yxf6zwqBMU09"
      },
      "outputs": [],
      "source": [
        "#Initializing Artificial Neural Network\n",
        "ann = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eOu0FhEMvfp"
      },
      "source": [
        "Step 10: Create two hidden layers with 6 neurons each and \"relu\" activation function. In the neural network, we define two hidden layers, each with 6 neurons. The \"relu\" (rectified linear unit) activation function is used to introduce non-linearity in the model, allowing it to learn complex patterns in the data. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "COiDlRZaMVH4"
      },
      "outputs": [],
      "source": [
        "#Adding First Hidden Layer\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n",
        "\n",
        "#Adding Second Hidden Layer\n",
        "ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZL2-EfvMVRl"
      },
      "source": [
        "Step 11: Create the output layer with 1 neuron and \"sigmoid\" activation function. The output layer has 1 neuron because we are performing binary classification (predicting one of two classes). The \"sigmoid\" activation function is used here to squash the output between 0 and 1, representing the probability of the positive class. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fDulrcuBMVck"
      },
      "outputs": [],
      "source": [
        "#Adding Output Layer\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDr5oH1CMVl3"
      },
      "source": [
        "Step 12: Compile the neural network using the \"adam\" optimizer, \"binary_crossentropy\" loss function, and \"accuracy\" as the performance metric. Before training the neural network, we need to compile it. Here, we specify the optimizer, loss function, and performance metric to be used during the training process. *Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "N8D89uOBM5hp"
      },
      "outputs": [],
      "source": [
        "#Compiling ANN\n",
        "ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYy8Gm-SSG34"
      },
      "source": [
        "Step 13: Fit the neural network on the training dataset with a batch size of 32 and 10 epochs. The final step is to train the neural network using the fit() method. We provide the training dataset, batch size (number of samples used in each update), and the number of epochs (number of times the model will iterate over the entire training dataset). During the training process, the neural network learns the optimal weights and biases to make accurate predictions on new data.*Nothing to change in the code below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "KoRpduwaNwQr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7149 - loss: 0.6217\n",
            "Epoch 2/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.7903 - loss: 0.4778\n",
            "Epoch 3/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.7934 - loss: 0.4468\n",
            "Epoch 4/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 739us/step - accuracy: 0.8082 - loss: 0.4241\n",
            "Epoch 5/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 696us/step - accuracy: 0.8100 - loss: 0.4281\n",
            "Epoch 6/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 0.8201 - loss: 0.4072\n",
            "Epoch 7/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - accuracy: 0.8248 - loss: 0.3997\n",
            "Epoch 8/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - accuracy: 0.8292 - loss: 0.3996\n",
            "Epoch 9/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 762us/step - accuracy: 0.8274 - loss: 0.3968\n",
            "Epoch 10/10\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740us/step - accuracy: 0.8281 - loss: 0.3926\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x255a4be76d0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Fitting ANN\n",
        "ann.fit(X_train, Y_train, batch_size=32, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FW4MO9_WmAK"
      },
      "source": [
        "Step 14. It's your turn to experiment what happens when you enter a set of inputs. You will need to enter 12 values that are reasonable for the datset. As an example:\n",
        "\n",
        "The values 1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1,50000\n",
        "would be a customer with a credit score of 600, female (1), 40 years old, from France (first three numbers are 1,0,0) who has been when the bank for 3 years, has a balance of 60,000, has 2 products, has a credit card (1), is an active member (1), and has an estimated salary of 50,000.\n",
        "\n",
        "Change the other numbers to see if you can find a customer who might leave (True)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AvknmL3cWmak"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "[[False]]\n"
          ]
        }
      ],
      "source": [
        "#Predicting result for Single Observation\n",
        "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1,50000]])) > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u2hN7pGcyZQ"
      },
      "source": [
        "Copy the code above and show 5 experiments with different inputs with at least 1 true prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z-F5fI4r-m9R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "[[ True]]\n"
          ]
        }
      ],
      "source": [
        "#Experiment 1\n",
        "print(ann.predict(sc.transform([\n",
        "    [\n",
        "        1, 0, 0, # country: France\n",
        "        500, # credit score\n",
        "        1, # gender\n",
        "        50, # age\n",
        "        1, # tenure\n",
        "        3000, # balance\n",
        "        1, # products\n",
        "        0, # credit card\n",
        "        0, # active member\n",
        "        30_000 # estimated salary\n",
        "    ]])) > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-pbqFGab-orU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "[[False]]\n"
          ]
        }
      ],
      "source": [
        "#Experiment 2\n",
        "\n",
        "print(ann.predict(sc.transform([\n",
        "    [\n",
        "        0, 1, 0, # country: Germany\n",
        "        600, # credit score\n",
        "        0, # gender\n",
        "        35, # age\n",
        "        5, # tenure\n",
        "        60000, # balance\n",
        "        1, # products\n",
        "        1, # credit card\n",
        "        1, # active member\n",
        "        110_000 # estimated salary\n",
        "    ]])) > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "i5-7sPKq-o0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "[[ True]]\n"
          ]
        }
      ],
      "source": [
        "#Experiment 3\n",
        "print(ann.predict(sc.transform([\n",
        "    [\n",
        "        0, 0, 1, # country: Spain\n",
        "        800, # credit score\n",
        "        1, # gender\n",
        "        45, # age\n",
        "        3, # tenure\n",
        "        0, # balance\n",
        "        0, # products\n",
        "        1, # credit card\n",
        "        0, # active member\n",
        "        101_000 # estimated salary\n",
        "    ]])) > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HQtzfJx7-pVx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "[[False]]\n"
          ]
        }
      ],
      "source": [
        "#Experiment 4\n",
        "print(ann.predict(sc.transform([\n",
        "    [\n",
        "        0, 1, 0, # country: Germany\n",
        "        850, # credit score\n",
        "        1, # gender\n",
        "        46, # age\n",
        "        1, # tenure\n",
        "        0, # balance\n",
        "        1, # products\n",
        "        1, # credit card\n",
        "        1, # active member\n",
        "        107_000 # estimated salary\n",
        "    ]])) > 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bgBKnmpl-wIi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "[[False]]\n"
          ]
        }
      ],
      "source": [
        "#Experiment 5\n",
        "print(ann.predict(sc.transform([\n",
        "    [\n",
        "        1, 0, 0, # country: France\n",
        "        850, # credit score\n",
        "        1, # gender\n",
        "        55, # age\n",
        "        30, # tenure\n",
        "        100, # balance\n",
        "        5, # products\n",
        "        4, # credit card\n",
        "        0, # active member\n",
        "        100_000 # estimated salary\n",
        "    ]])) > 0.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
